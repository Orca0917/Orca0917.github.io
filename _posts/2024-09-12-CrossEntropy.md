---
title: 교차 엔트로피와 KL 발산 - 질문으로 이해하기
date: 2024-09-12 13:25:00 +0900
categories: [Deep Learning, Basic]
tags: [entropy, cross entropy, kl divergence]     # TAG names should always be lowercase
author: moon
math: true
toc: true
---

머신러닝을 공부하다 보면 자주 등장하는 두 개념이 있습니다. 바로 **교차 엔트로피(Cross Entropy)**와 **쿨백-라이블러(KL) 발산**입니다. 그런데 이 두 개념이 서로 비슷해 보이면서도, 실제로는 어떻게 다르고, 각각 어떤 상황에서 사용되는지 헷갈릴 수 있습니다. 그래서 이번 글에서는 의문을 가졌던 부분에 대해서 질의하며 교차 엔트로피와 KL 발산을 하나씩 풀어보겠습니다.

<br>

## 질문 1: 교차 엔트로피 이전에 엔트로피(Entropy)란 게 뭔가요?

**엔트로피(Entropy)**는 기본적으로 **정보의 불확실성**을 나타내는 개념입니다. 즉, 어떤 사건이 일어날지 얼마나 예측하기 어려운지, 그 불확실성을 수치화한 것이 엔트로피입니다. 이 개념은 원래 **정보 이론**에서 나왔습니다. 간단히 말해, **엔트로피 값이 클수록 정보가 불확실**하고, **엔트로피 값이 작을수록 예측하기 쉬운 정보**라는 뜻입니다.

### 엔트로피의 예시:

주사위를 던진다고 생각해봅시다. 주사위에는 1부터 6까지의 숫자가 나올 수 있죠. 주사위를 던졌을 때 **각 숫자가 나올 확률은 1/6**입니다. 이때, **어떤 숫자가 나올지 정확히 예측하기 어려운 상황**이므로, 엔트로피는 높습니다. 반면에, 동전 던지기에서 앞면이 나올 확률이 99%라면, **앞면이 나올 확률이 매우 높아 예측이 쉽습니다**. 이 경우 엔트로피는 매우 낮습니다.

### 엔트로피 공식:

$$
H(P) = - \sum_{i} P(i) \log P(i)
$$

- $P(i)$: 각 사건 $i$가 발생할 확률입니다.
- 로그 함수는 확률이 작을수록 더 큰 음수 값을 반환하는데, 이 값에 마이너스를 붙이면 불확실성을 양수로 계산할 수 있습니다.

이렇게 **엔트로피는 사건의 불확실성을 수치로 나타내는 도구**라고 생각하면 됩니다. 엔트로피가 클수록 예측이 어렵고, 작을수록 예측이 쉬운 상황을 나타내죠.

<br>

## 질문 2: 그럼 엔트로피와 교차 엔트로피는 어떻게 연결되나요?

엔트로피가 **한 분포 내에서의 불확실성**을 측정하는 것이라면, **교차 엔트로피(Cross Entropy)**는 **두 확률 분포 간의 차이**를 측정하는 방식입니다. 즉, **실제 정답 확률 분포 $P$**와 **모델이 예측한 확률 분포 $Q$**가 얼마나 다른지 평가하는 것이 교차 엔트로피입니다. 
교차 엔트로피는 모델이 얼마나 정확하게 예측했는지를 측정하는 데 유용합니다. **예측이 정답에 가까울수록 교차 엔트로피 값은 작아지고**, **예측이 틀릴수록 값은 커집니다**.

### 교차 엔트로피 공식:

$$
H(P, Q) = - \sum_{i} P(i) \log Q(i)
$$

- $P(i)$: 실제 정답 클래스에 대한 확률.
- $Q(i)$: 모델이 예측한 해당 클래스의 확률.

<br>

> 여기서 중요한 것은 정답 분포인 $P(i)$ 는 보통 0 또는 1의 라벨값을 갖게됩니다. 이는 위의 식에서 실제로 정답인 것의 예측값으로만 손실값 (교차 엔트로피)가 계산됨을 의미하기도 합니다. 따라서, 우리는 교차엔트로피가 **정답값에 대해서만 두 확률분포의 차이를 계산**하는 손실함수라고 이해할 수 있습니다.
{: .prompt-info}

<br>

## 질문 3: 교차 엔트로피에서 왜 로그를 쓰죠? 그냥 $P(i) \times Q(i)$로 계산하면 안 되나요?

교차 엔트로피에서 **로그**를 사용하는 이유는 **예측이 틀렸을 때 더 큰 페널티**를 주기 위해서입니다. 만약 단순히 $P(i) \times Q(i)$로 계산하면, 모델이 예측을 틀리더라도 손실이 크게 반영되지 않습니다. 로그 함수는 확률이 1에 가까울수록 값이 0에 가깝고, 확률이 0에 가까울수록 큰 음수 값을 가집니다. 따라서 모델이 틀리면 **로그 값이 크게 나와 큰 손실**이 발생하고, 맞을 때는 **작은 손실**이 발생합니다.

### 예를 들어:
- **정답이 1인 상황에서** 모델이 0.9로 예측하면 손실이 작지만, 0.1로 예측하면 큰 손실이 발생합니다. 
- 이렇게 로그 함수는 예측이 정확할수록 손실을 적게 주고, 예측이 틀릴수록 손실을 크게 줍니다.

<br>

## 질문 4: KL 발산(KL Divergence)은 뭐가 다른가요?

**KL 발산(KL Divergence)**도 두 확률 분포 간의 차이를 계산하지만, **전체 확률 분포의 차이**를 평가합니다. 교차 엔트로피가 **정답 클래스에 집중**하는 것과 달리, **KL 발산은 전체 분포에서 모델이 얼마나 잘 예측했는지**를 평가합니다.

### KL 발산 공식:

$$
\begin{aligned}
D_{\text{KL}}(P \parallel Q) &= \sum_{i} P(i) \log \left( \frac{P(i)}{Q(i)} \right) \\

&= \sum_i P(i) \log P(i) - P(i) \log Q(i) \\

&= - H(P) + H(P, Q)
\end{aligned}
$$

이 공식은 **실제 분포 $P$**와 **예측 분포 $Q$**가 전체적으로 얼마나 다른지를 계산합니다. 

<br>

## 질문 5: KL 발산에서 정답 분포의 엔트로피를 빼는 이유는 뭔가요?

**KL 발산**은 사실 **교차 엔트로피에서 정답 분포의 엔트로피를 뺀 값**으로 표현할 수 있습니다. 그 이유는 **정답 분포가 예측하기 얼마나 어려운지를 반영**하기 위해서입니다.

$$
D_{\text{KL}}(P \parallel Q) = H(P, Q) - H(P)
$$

- **$H(P, Q)$**: 교차 엔트로피로, 모델이 예측한 분포가 실제 정답과 얼마나 다른지 측정합니다.
- **$H(P)$**: 실제 정답 분포의 엔트로피로, 정답이 본래 얼마나 불확실한지를 나타냅니다.

**KL 발산은 정답 분포의 불확실성을 고려하여 전체적으로 두 분포가 얼마나 다른지 평가**합니다. 예측이 어려운 상황에서는 틀렸을 때 페널티가 작고, 예측이 쉬운 상황에서는 페널티가 큽니다.

<br>

## 질문 6: 쉽게 설명해 줄 수 있나요? (비유를 통한 이해)

시험을 본다고 생각해봅시다.

- **쉬운 문제**: 대부분의 학생이 맞출 수 있는 문제입니다. 이 문제를 틀리면 교사가 **큰 페널티**를 줍니다. 이는 **정답 분포가 명확할 때** 모델이 틀리면 큰 페널티를 주는 것과 같습니다.
  
- **어려운 문제**: 맞추기 어려운 문제입니다. 이 문제를 틀려도 페널티가 크지 않습니다. 이는 **정답 분포가 불확실할 때** 모델이 틀리더라도 작은 페널티를 주는 KL 발산의 특성과 비슷합니다.

<br>

## 질문 7: 결론적으로, 교차 엔트로피와 KL 발산의 차이점은 무엇인가요?

**교차 엔트로피**는 **정답 클래스에 집중**해서 예측이 얼마나 틀렸는지 평가하는 방식입니다. 반면, **KL 발산**은 **전체 확률 분포**의 차이를 평가하면서 **정답 분포의 난이도**를 반영합니다. 두 개념 모두 확률 분포 간의 차이를 계산하지만, **교차 엔트로피는 정답에 집중**하고, **KL 발산은 전체 분포 차이를 평가**하는 데 차이가 있습니다.

---

<br>

이 글을 통해 **엔트로피, 교차 엔트로피, KL 발산**의 차이와 그 의미를 더 쉽게 이해할 수 있었기를 바랍니다. 😊