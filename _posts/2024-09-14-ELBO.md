---
title: "잠재 변수 모델을 더 쉽게: ELBO의 역할과 유도 과정"
date: 2024-09-14 16:35:00 +0900
categories: [Deep Learning, Basic]
tags: [elbo]     # TAG names should always be lowercase
author: moon
math: true
toc: true
---

현대의 인공지능 모델은 점점 더 복잡해지고, 우리가 다루는 데이터도 그만큼 다양해지고 있습니다. 그중 **잠재 변수 모델**은 보이지 않는 정보를 활용해 데이터를 더 잘 설명하려고 합니다. 하지만, 이 복잡한 계산을 효율적으로 처리하기 위해서는 무엇이 필요할까요? 바로 **ELBO**라는 도구가 그 답입니다. ELBO는 복잡한 확률 계산을 간단하게 풀어주고, 모델이 더 나은 성능을 내도록 돕는 강력한 방법론입니다. 

이 글에서는 ELBO가 무엇인지, 그리고 이를 통해 **복잡한 계산을 어떻게 간단하게 만드는지** 차근차근 설명하겠습니다. 먼저, ELBO의 수학적 개념과 그 유도 과정을 살펴보며, 중요한 수식과 개념을 쉽게 이해할 수 있도록 설명하겠습니다.

<br>

## 1. 로그 우도 $\log p(x)$ 와 잠재 변수 $z$ 

우리가 구하고 싶은 것은 **로그 우도** $\log p(x)$ 입니다. 이는 모델이 주어진 데이터 $x$ 가 발생할 확률을 계산한 것 입니다. 수식으로 표현하면 다음과 같습니다:

$$
\log p(x) = \log(\text{데이터 }x\text{가 나올 확률})
$$

하지만 데이터를 더 잘 설명하기 위해 **잠재 변수 $z$** 를 도입할 때가 많습니다. 예를 들어, 음성 데이터에 숨겨진 정보를 설명하기 위해 잠재 변수(감정, 소리의 세기, 빠르기)가 필요할 수 있습니다. 이때 **잠재 변수 $z$** 를 고려해야 하기 때문에 로그 우도는 다음과 같은 적분 형태로 재표현됩니다:

$$
p(x) = \int p(x, z) \, dz
$$

여기서 $p(x, z)$ 는 데이터 $x$ 와 잠재 변수 $z$ 의 **결합 확률**입니다. $z$ 는 우리가 관측할 수 없는 숨겨진 정보로, 모든 $z$ 의 가능한 값을 고려하여 적분을 수행합니다. 하지만 이 적분을 직접 계산하는 것은 매우 어렵기 때문에, 이를 단순화하기 위해 **근사 분포 $q_\theta(z)$**를 도입합니다.

<br>

## 2. 근사 분포 $q_\theta(z)$  도입

**$q_\theta(z)$** 는 잠재 변수 $z$ 에 대한 근사 분포로, 이를 통해 복잡한 계산을 더 쉽게 할 수 있습니다. 여기서 $\theta$ 는 **모델의 파라미터**로, 우리가 학습해야 하는 대상입니다. 근사 분포 $q_\theta(z)$ 는 복잡한 $p(x)$ 를 직접 계산하는 대신, 이를 근사하는 분포로 사용됩니다. 우리는 **$p(x)$**를 구하기 위해 다음과 같은 트릭을 사용하게 됩니다: 근사 분포 $q_\theta(z)$ 를 곱하고 나눔으로써 적분을 기댓값 형태로 표현합니다:

$$
p(x) = \int \frac{p(x, z)}{q_\theta(z)} q_\theta(z) \, dz
$$

이 식에서 $q_\theta(z)$ 는 우리가 학습할 근사 분포입니다. 이제 이 적분을 기댓값으로 변환할 수 있습니다.

<br>

## 3. 적분의 기댓값 변환

이제 적분을 기댓값으로 변환하는 과정을 설명하겠습니다. 적분은 특정 확률 분포에 대해 평균을 구하는 방식으로 기댓값과 동일한 역할을 합니다.

### 3.1 적분과 기댓값의 관계

먼저 **적분**과 **기댓값**의 관계를 간단한 예시로 설명해보겠습니다. 우리가 특정 분포 $q(z)$ 를 알고 있고, 이 분포에 대해 함수 $f(z)$ 의 평균을 구하려고 한다고 가정해봅시다. 이를 수식으로 적분을 통해 나타내면 다음과 같습니다:

$$
\int q(z) f(z) \, dz
$$

이 적분은 분포 $q(z)$ 를 기준으로 함수 $f(z)$ 의 값을 평균 내는 과정입니다. **적분**은 특정 범위에서 함수를 합산하는 것이기 때문에, 이 과정을 **기댓값**으로 변환할 수 있습니다. 적분을 기댓값으로 표현하면 이렇게 됩니다:

$$
\mathbb{E}_{q(z)}[f(z)] = \int q(z) f(z) \, dz
$$

즉, *적분은 특정 확률 분포 $q(z)$ 에 대해 **기댓값**을 구하는 과정과 동일합니다.*

<br>

### 3.2 ELBO에서의 적분과 기댓값 변환

이제 이 개념을 가지고, 복잡한 로그 우도를 계산하는 과정을 간단하게 만들 수 있습니다. 위에서 잠재 변수 $z$ 를 고려한 로그 우도 $p(x)$ 를 구하기 위해 도입한 근사 분포 $q_\theta(z)$ 를 사용하여, 적분을 기댓값으로 변환해봅시다:

$$
p(x) = \int \frac{p(x, z)}{q_\theta(z)} q_\theta(z) \, dz = \mathbb{E}_{q_\theta(z)} \left[ \frac{p(x, z)}{q_\theta(z)} \right]
$$

여기서 $\mathbb{E}\_{q\_\theta(z)}$ 는 **근사 분포 $q_\theta(z)$** 에 대한 **기댓값**을 의미합니다. 이제 적분을 직접 계산하지 않고 기댓값으로 변환하여, 훨씬 간단한 방식으로 문제를 다룰 수 있습니다.

<br>

## 4. 젠슨의 부등식 (Jensen's Inequality) 적용

젠슨의 부등식은 기댓값을 더 쉽게 다룰 수 있게 해주는 수학적 도구입니다. 젠슨의 부등식은 다음과 같이 표현할 수 있습니다:

$$
\log \mathbb{E}[X] \geq \mathbb{E}[\log X]
$$

이 부등식을 위의 식에 적용하면, 로그를 기댓값 안에서 계산하는 것이 아니라, 로그를 먼저 취하고 기댓값을 구하게 됩니다. 이를 통해 계산을 단순화할 수 있습니다. 이 과정을 적용하면 다음과 같은 결과를 얻게 됩니다:

$$
\log p(x) = \log \mathbb{E}_{q_\theta(z)} \left[ \frac{p(x, z)}{q_\theta(z)} \right] \geq \mathbb{E}_{q_\theta(z)} \left[ \log \frac{p(x, z)}{q_\theta(z)} \right]
$$

이 식에서 오른쪽에 있는 값이 바로 **ELBO**입니다. ELBO는 $\log p(x)$ 의 하한선을 제공하며, 이를 통해 우리가 계산을 더 쉽게 할 수 있습니다.

<br>

## 5. ELBO의 최종 형태

ELBO의 최종 형태는 다음과 같습니다:

$$
\log p(x) \geq \mathbb{E}_{q_\theta(z)} \left[ \log p(x, z) - \log q_\theta(z) \right]
$$

여기서:
- $\mathbb{E}\_{q\_\theta(z)}$ : 잠재 변수 $z$ 에 대한 근사 분포 $q_\theta(z)$ 에 대한 기댓값
- $\log p(x, z)$ : 데이터 $x$ 와 잠재 변수 $z$ 의 결합 확률의 로그
- $\log q_\theta(z)$ : 잠재 변수 $z$ 의 근사 분포 $q_\theta(z)$ 의 로그

이 식이 바로 우리가 구한 **ELBO**입니다. ELBO는 $\log p(x)$ 의 하한선이며, 이를 최대화하는 것이 모델의 학습 목표가 됩니다.

<br>

## 6. ELBO의 의미

_**ELBO**를 최대화하는 것은 결국 우리가 구하고 싶은 로그 우도를 최대화하는 것과 비슷한 효과를 줍니다. 즉, 데이터를 가장 잘 설명하는 모델을 찾으려면, 우리는 ELBO를 최대화해야 합니다._ ELBO는 복잡한 로그 우도 계산을 단순화하면서 그 하한선을 제공하는 매우 중요한 도구입니다.

<br>

## 7. 정리

ELBO는 **로그 우도**를 복잡한 계산 없이 간단하게 근사할 수 있는 도구입니다. 우리는 **근사 분포 $q_\theta(z)$** 를 도입하여 적분을 기댓값으로 변환하고, **젠슨의 부등식**을 적용하여 로그 우도의 하한을 구할 수 있습니다. ELBO는 이 하한을 최대화하여 데이터를 잘 설명할 수 있는 모델을 학습하는 데 중요한 역할을 합니다. ELBO는 복잡한 확률 모델에서 중요한 계산 도구이며, 이를 통해 데이터에 숨겨진 정보까지 고려하는 잠재 변수 모델을 효과적으로 다룰 수 있게 됩니다.