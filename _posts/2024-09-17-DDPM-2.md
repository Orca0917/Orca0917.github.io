---
title: "DDPM 논문 이해하기 (2편): Forward 및 Reverse Process에 대한 이해"  
date: 2024-09-17 22:25:00 +0900  
categories: [Computer Vision, Generative Model]  
tags: [markov chain, diffusion]  # TAG names should always be lowercase  
author: moon  
math: true  
toc: true  
---

[[PDF](https://arxiv.org/pdf/2006.11239)] [[Code](https://github.com/hojonathanho/diffusion)]

지난 1편에서는 DDPM에 대한 전반적인 개요와 모델의 한계에 대해 설명했습니다. 이번 2편에서는 핵심적인 과정인 노이즈 추가를 다루는 **forward process**와 노이즈 제거를 다루는 **reverse process**에 대해 수식과 함께 자세히 알아보겠습니다.

<br>

## 1. Forward Process (확산 과정)

DDPM의 **forward process**는 데이터 $\mathbf{x}_0$에 점진적으로 **가우시안 노이즈**를 추가하는 과정입니다. 이 과정은 **마코프 체인**으로 정의되며, 각 단계에서 분산 $\beta_t$에 따라 노이즈가 더해집니다. Forward process의 근사 사후 분포는 다음과 같습니다:

$$
q(\mathbf{x}_{1:T}|\mathbf{x}_{0}) := \prod_{t=1}^{T}q(\mathbf{x}_{t}|\mathbf{x}_{t-1}), \quad q(\mathbf{x}_{t}|\mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_{t}; \sqrt{1-\beta_{t}}\mathbf{x}_{t-1}, \beta_{t}\mathbf{I})
$$

여기서 $\beta_t$는 각 단계에서 추가되는 노이즈의 분산을 나타내며, $\mathbf{x}_t$는 노이즈가 추가된 데이터입니다. 시간이 지남에 따라 원본 데이터는 점차 노이즈로 덮여가고, 마지막 단계 $T$에 도달하면 $\mathbf{x}_T$는 거의 **가우시안 노이즈**에 가까워집니다.

![graphics - diffusion](/assets/img/ddpm/graphical-model.png)

<br>

## 2. 왜 $\sqrt{1 - \beta_t}$ 라는 값이 나오는가?

DDPM에서 **$\sqrt{1 - \beta_t}$**라는 스케일링 계수는 **forward diffusion 과정에서 분산이 일정하게 유지되도록** 설계되었습니다. 이를 이해하려면 각 단계에서 노이즈 추가 시 분산이 어떻게 변하는지 살펴볼 필요가 있습니다.

<br>

### 2.1. 스케일링 계수의 필요성

Forward diffusion 과정에서 원본 데이터 $\mathbf{x}\_0$에 점진적으로 노이즈가 추가됩니다. 이때, 노이즈가 누적되면 분산이 과도하게 증가할 수 있습니다. 만약 각 단계에서 $\mathbf{x}\_{t-1}$을 적절히 조정하지 않으면 **분산이 폭발**할 가능성이 큽니다. 이로 인해 모델 성능이 크게 저하될 수 있죠.

예를 들어, 초기 데이터 $\mathbf{x}_0$의 분산이 1로 정규화되었다고 가정해도, 시간이 지남에 따라 노이즈가 계속해서 더해지면 최종 단계 $\mathbf{x}_T$의 분산은 매우 커질 수 있습니다. 이를 방지하기 위해 **분산을 조절할 수 있는 스케일링 계수**가 필요합니다.

<br>

### 2.2. 스케일링 계수의 유도

각 단계에서 분산을 일정하게 유지하려면 어떻게 해야 할까요? 첫 번째 단계에서 $\mathbf{x}_1$은 다음과 같이 정의됩니다:

$$
\mathbf{x}_1 = a \mathbf{x}_0 + \sqrt{\beta_1} \epsilon_1 \quad \text{where, } \mathbf{x}_0\sim\mathcal{N}(0, 1), \epsilon_1 \sim \mathcal{N}(0, 1)
$$

여기서 $a$는 스케일링 계수, $\epsilon_1$은 추가된 노이즈입니다. $\mathbf{x}_1$의 분산은 다음과 같이 계산됩니다:

$$
\text{Var}(\mathbf{x}_1) = a^2 \cdot \text{Var}(\mathbf{x}_0) + \beta_1 \cdot \text{Var}(\epsilon_1) = a^2 + \beta_1
$$

우리가 원하는 것은 $\mathbf{x}_1$의 분산이 $\mathbf{x}_0$와 동일하게 1로 유지되는 것입니다. 따라서:

$$
a^2 + \beta_1 = 1
$$

이 식을 풀면 스케일링 계수 $a$는 다음과 같이 결정됩니다:

$$
a = \sqrt{1 - \beta_1}
$$

이 원리는 모든 단계 $t$에서도 동일하게 적용됩니다. 따라서, 각 단계에서 $\mathbf{x}_{t-1}$을 **$\sqrt{1 - \beta_t}$**로 스케일링한 후 노이즈를 추가하게 됩니다. 이를 통해 각 단계의 분산을 일정하게 유지하며, 노이즈 축적에 따른 분산 폭발을 방지할 수 있습니다.

<br>

### 2.3. 결과

이 스케일링 계수를 적용하면 DDPM의 forward process는 다음과 같은 형태를 갖습니다:

$$
\mathbf{x}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1} + \sqrt{\beta_t} \epsilon_t
$$

이 식은 각 단계마다 노이즈를 추가할 때 **$\sqrt{1 - \beta_t}$**라는 계수를 사용해 데이터의 분산이 안정적으로 유지되도록 합니다. 이를 통해 모델이 안정적으로 학습할 수 있으며, **확률 분포**는 다음과 같이 정의됩니다:

$$
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
$$

또한, **임의의 시간 단계 $t$에서** $\mathbf{x}\_t$를 폐쇄형 형태로 샘플링할 수 있습니다. 이를 위해, $\alpha\_t := 1-\beta\_t$와 $\bar{\alpha}\_t := \prod\_{s=1}^{t}\alpha\_s$를 정의하면:

$$
q(\mathbf{x}_{t}|\mathbf{x}_{0}) = \mathcal{N}(\mathbf{x}_{t}; \sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}, (1-\bar{\alpha}_{t})\mathbf{I})
$$

이 식을 사용하면 초기 데이터 $\mathbf{x}_0$에서 임의의 시간 $t$에 해당하는 노이즈가 추가된 데이터를 바로 계산할 수 있습니다.

<br>

## 3. Reverse Process (역방향 과정)

**Reverse process**는 forward process에서 노이즈가 추가된 데이터를 다시 원래 데이터로 복원하는 과정입니다. 이 역시 **마코프 체인**으로 정의되며, 각 단계에서 **가우시안 조건부 분포**를 학습해 노이즈를 제거합니다. 역방향 과정의 결합 분포는 다음과 같습니다:

$$
p_{\theta}(\mathbf{x}_{0:T}) := p(\mathbf{x}_{T})\prod_{t=1}^{T} p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t}), \quad p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t}) := \mathcal{N}(\mathbf{x}_{t-1}; {\boldsymbol{\mu}}_{\theta}(\mathbf{x}_{t},t), {\boldsymbol{\Sigma}}_{\theta}(\mathbf{x}_{t},t))
$$

여기서 $p(\mathbf{x}_T)$는 가우시안 분포로 초기화되고, $\mathbf{x}_T$부터 시작해 각 단계를 거쳐 데이터가 복원됩니다. 중요한 점은, **$\beta_t$가 작은 경우 forward와 reverse 과정이 동일한 함수 형태**를 가진다는 것입니다.

<br>

## 4. 마무리

이번 글에서는 DDPM의 **forward 및 reverse process**에 대해 수식과 함께 자세히 살펴보았습니다. 이를 이해하면 확산 모델의 작동 원리를 더욱 명확하게 알 수 있으며, 다양한 응용에 확산 모델을 활용하는 데 도움이 될 것입니다.