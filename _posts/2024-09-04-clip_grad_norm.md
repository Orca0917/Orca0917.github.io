---
title: 딥러닝에서 기울기 클리핑(Gradient Clipping)이란? PyTorch의 `clip_grad_norm_` 함수 이해하기
date: 2024-09-04 14:51:00 +0900
categories: [PyTorch, Functions]
tags: [PyTorch]     # TAG names should always be lowercase
author: moon
math: true
toc: true
---

딥러닝에서 학습을 진행할 때, 특히 RNN(LSTM, GRU 등)과 같은 순환 신경망을 다룰 때, 우리는 **기울기 폭발(Gradient Explosion)**이라는 문제를 종종 마주하게 됩니다. 이 문제는 기울기의 크기가 너무 커지면서 학습이 불안정해지고, 모델이 제대로 학습되지 않는 현상을 말합니다. 이를 해결하기 위해 **기울기 클리핑(Gradient Clipping)**이라는 방법을 사용할 수 있습니다.

이번 글에서는 PyTorch에서 제공하는 `clip_grad_norm_` 함수를 사용해 기울기를 클리핑하는 방법과, 그 원리 및 활용법을 쉽게 설명해보겠습니다.

<br>

## 1. 기울기 폭발

딥러닝에서 신경망을 학습할 때, 특히 **순환 신경망(RNN)**과 같은 구조에서는 **기울기 폭발(Gradient Explosion)**이 자주 발생합니다. 이는 네트워크가 깊어지거나, 시퀀스 길이가 길어질수록 *역전파 중에 기울기가 매우 커지면서 파라미터 업데이트가 지나치게 크게 되는 문제*입니다. 이로 인해 모델이 수렴하지 않거나, 매우 불안정하게 학습되는 상황이 발생합니다.

기울기 폭발은 학습이 **너무 느리거나 실패하는 원인**이 될 수 있기 때문에, 이를 방지하기 위한 대책이 필요합니다. 이를 해결하는 대표적인 방법이 바로 **기울기 클리핑(Gradient Clipping)**입니다.

<br>

## 2. 기울기 클리핑

기울기 클리핑은 기울기가 일정 임계값 이상으로 커지는 것을 방지하기 위해, 기울기의 크기를 제한하는 방법입니다. 이를 통해 **너무 큰 기울기를 제한**하여, 기울기 폭발로 인한 불안정한 학습을 막을 수 있습니다.

기울기 클리핑의 주요 아이디어는 기울기의 크기가 **주어진 임계값**을 초과할 때, 그 크기를 **조정하여 임계값을 넘지 않도록** 만드는 것입니다. 이를 통해 모델이 보다 안정적으로 학습되도록 할 수 있습니다.

<br>

## 3. torch.nn.utils.clip_grad_norm_ 함수

PyTorch에서는 기울기 클리핑을 간단하게 구현할 수 있도록 `torch.nn.utils.clip_grad_norm_`이라는 함수를 제공합니다. 이 함수는 모델의 파라미터에서 계산된 기울기를 특정 노름(norm) 기준으로 클리핑해줍니다.

<br>

### 함수 정의

```python
torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2)
```

- **`parameters`**: 클리핑할 대상인 파라미터들. 주로 `model.parameters()`로 전달됩니다.
- **`max_norm`**: 기울기의 최대 노름 값. 기울기가 이 값을 넘으면 클리핑됩니다.
- **`norm_type`**: 사용할 노름의 종류(L2 노름 등). 기본값은 `2`로, L2 노름을 사용합니다.

<br>

### 기울기 클리핑의 동작 방식

1. **기울기 계산**: 역전파(backpropagation)를 통해 각 파라미터의 기울기를 계산합니다.
2. **노름 계산**: 모든 파라미터의 기울기에 대해 주어진 노름(L2 노름 등)을 계산합니다.
3. **기울기 클리핑**: 만약 기울기의 노름이 `max_norm`을 초과하면, 기울기의 크기를 임계값에 맞게 스케일링하여 조정합니다.
4. **기울기 적용**: 이후 기울기를 사용해 모델의 파라미터를 업데이트합니다.

<br>

## 4. 코드 예시: `clip_grad_norm_` 사용 방법

아래 예시는 PyTorch에서 기울기 클리핑을 적용하는 간단한 코드입니다:

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 임의의 모델 정의
model = nn.Linear(10, 2)

# 손실 함수 및 옵티마이저 정의
loss_fn = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 임의의 입력과 출력
input = torch.randn(5, 10)
target = torch.randn(5, 2)

# 순전파
output = model(input)
loss = loss_fn(output, target)

# 역전파
optimizer.zero_grad()
loss.backward()

# 기울기 클리핑 적용 (순서 중요!!)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 파라미터 업데이트
optimizer.step()
```

<br>

### 코드 설명:
1. `model.parameters()`는 클리핑할 모델의 파라미터를 의미합니다.
2. `max_norm=1.0`은 기울기의 최대 노름 값을 1로 제한한다는 의미입니다. 즉, 기울기의 L2 노름이 1을 넘지 않도록 클리핑이 발생합니다.
3. `clip_grad_norm_`은 파라미터의 기울기들이 `max_norm`을 넘지 않도록 조정하여, 학습 과정에서 기울기 폭발을 방지합니다.

<br>

## 5. 기울기 클리핑의 장점

1. **기울기 폭발 방지**: 기울기가 지나치게 커지는 문제를 해결하여, 학습이 비정상적으로 커지지 않도록 안정성을 제공합니다.
2. **모델 성능 향상**: 기울기 폭발을 방지함으로써, 모델이 더 빠르게 수렴할 수 있도록 돕습니다.
3. **학습 안정성 강화**: 지나치게 큰 기울기 변화로 인해 모델이 갑작스럽게 잘못 학습되는 문제를 방지합니다.

<br>

## 6. 기울기 클리핑의 활용 시점

기울기 클리핑은 주로 **순환 신경망(RNN, LSTM, GRU)**과 같은 네트워크에서 사용됩니다. 이러한 모델들은 시퀀스 데이터를 처리하며, 기울기 폭발 문제가 자주 발생하기 때문에 기울기를 안정적으로 유지하는 것이 중요합니다. 그러나 CNN이나 MLP와 같은 다른 신경망에서도 학습이 불안정하다면 기울기 클리핑을 적용할 수 있습니다.

<br>

## 7. 결론

기울기 폭발은 딥러닝 모델의 학습을 방해하는 대표적인 문제 중 하나입니다. 이를 해결하기 위한 효과적인 방법인 **기울기 클리핑**은 PyTorch에서 제공하는 `clip_grad_norm_` 함수를 사용하여 쉽게 구현할 수 있습니다. 이제 기울기 클리핑을 통해 모델 학습의 안정성을 높이고, 기울기가 너무 커지지 않도록 제한하여 학습 과정에서 발생할 수 있는 문제들을 사전에 방지할 수 있습니다.

- `torch.nn.utils.clip_grad_norm_` : 언더스코어(_)가 붙은 함수는 **인플레이스(in-place) 방식**으로 동작합니다. 즉, 기울기를 직접 수정하여 해당 파라미터 텐서의 값이 바로 변경됩니다.
- `torch.nn.utils.clip_grad_norm`  : 언더스코어(_)가 붙지 않은 함수는 **인플레이스가 아닌(non-in-place) 방식**으로 동작합니다. 즉, 기울기를 클리핑한 새로운 값을 복사본으로 생성하고, 원래 파라미터 텐서의 값은 그대로 유지됩니다.